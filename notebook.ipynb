{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag it\n",
    "\n",
    "This is a multi-label classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/msr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/msr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/msr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"minutes\", \"hours\", \"seconds\", \"teaspoon\", \"spoon\"])\n",
    "\n",
    "punctuations = '!\"#$%&\\'()*+,.-/:;<=>?@[\\\\]^_`{|}~'\n",
    "table = str.maketrans('', '', punctuations)\n",
    "\n",
    "import difflib\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Input, Model\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, \\\n",
    "    GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate, concatenate, \\\n",
    "        TimeDistributed, MaxPooling1D, add\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"recipes_82k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before diving into the code - an explanation for some of my design choices \n",
    "\n",
    "###### Tokenisation, lemmatisation and general data cleaning - \n",
    "After trials, I finally settled on NLTK and its corpus instead of SpaCy. The primary reason was the time constraint and the fact that NLTK was more suited to rapid experimentation as it was more than twice as fast as SpaCy.\n",
    "\n",
    "###### Word vectors - \n",
    "I trained word vectors from the corpus itself, as this task is domain-specific, so I wasn't sure how a general-purpose word embedding scheme would perform.\n",
    "I used FastText embeddings in particular as they handle out-of-vocabulary words better than GloVe or Word2Vec.\n",
    "\n",
    "You can download my pretrained word embeddings from here - https://drive.google.com/open?id=17MdIgSJ3J9hupjzdEOJZ3e4sQBf8xaj0 and place the 3 files in a directory called word_vectors.\n",
    "\n",
    "You can also choose to train them from scratch by setting the appropriate flag during class instantiation.\n",
    "\n",
    "###### Classifier - \n",
    "I used a Bidirectional LSTM-based classifier constructed from the Keras functional (API). I experimented with Conv1D and multi-input models but, in most cases, I got similar results with added computational overhead. So I went with a simpler model in the end.\n",
    "\n",
    "###### Loss function - \n",
    "Binary crossentropy was used since the output probability of one tag is independent of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the class, methods and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGetter():\n",
    "    \n",
    "    \"\"\" This is a class that loads and transforms data so it can be \n",
    "    immediately usable with just the appropriate method calls.\n",
    "    \n",
    "    Also has attributes that contain extracted data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, training=True, vectors_pretrained=True):\n",
    "        \n",
    "        self.training = training\n",
    "        self.vectors_pretrained = vectors_pretrained\n",
    "        \n",
    "        if vectors_pretrained:\n",
    "            self.vectors_file = 'word_vectors/fasttext.model'\n",
    "         \n",
    "        # Drop unwanted variables\n",
    "        df.drop([\"category\", \"image\"], inplace=True, axis=1)\n",
    "        \n",
    "        # Handling NA and Null values\n",
    "        df['cuisine'].fillna('UNK_Cuisine', inplace=True)\n",
    "        df['prep_time'].fillna('UNK_Time', inplace=True)\n",
    "        df['serves'].fillna('UNK_Serves', inplace=True)\n",
    "        df.dropna(inplace = True)\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        # Handling duplicate values\n",
    "        df = df[df.duplicated() == False]\n",
    "        \n",
    "        # Shuffling the dataset for randomness\n",
    "        df = df.sample(frac=1, random_state=1234)\n",
    "        \n",
    "        # Extracting the fields from the data\n",
    "        self.cooking_method = df['cooking_method']\n",
    "        self.cuisine = list(df['cuisine'])\n",
    "        self.ingredients = list(df['ingredients'])\n",
    "        self.prep_time = list(df['prep_time'])\n",
    "        self.recipe_name = list(df['recipe_name'])\n",
    "        self.serves = list(df['serves'])\n",
    "        self.tags = list(df['tags'])\n",
    "        self.tags = [str(tag).split(\",\") for tag in self.tags]\n",
    "        self.tags_flat = [tag for tag_list in self.tags for tag in tag_list]  \n",
    "        self.tags_unique = list(set(self.tags_flat))\n",
    "        \n",
    "    \n",
    "    def preprocess_inputs(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        Method to pre-process the extracted fields (tokenisation, lemmatisation and general text cleaning)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Preprocessing cooking method text\n",
    "        self.tok_cooking_method = []\n",
    "        for elem in tqdm(self.cooking_method):\n",
    "            elem = elem.translate(table)\n",
    "            elem = lemmatizer.lemmatize(elem)\n",
    "\n",
    "            elem = word_tokenize(elem)\n",
    "            elem = [word.lower() for word in elem if word not in stop_words and word.isalpha() and not word.endswith(\"ly\")]\n",
    "            self.tok_cooking_method.append(elem)\n",
    "\n",
    "            \n",
    "        # Preprocessing ingredients text\n",
    "        self.tok_ingredients = []\n",
    "        for elem in tqdm(self.ingredients):\n",
    "            elem = elem.translate(table)\n",
    "            elem = lemmatizer.lemmatize(elem)\n",
    "            elem = word_tokenize(elem)\n",
    "            elem = [word.lower() for word in elem if word not in stop_words and word.isalpha() and not word.endswith(\"ly\")]\n",
    "            self.tok_ingredients.append(elem)\n",
    "\n",
    "        \n",
    "        # Preprocessing recipe name text\n",
    "        self.tok_recipe_name = []\n",
    "        for elem in tqdm(self.recipe_name):\n",
    "            elem = elem.translate(table)\n",
    "            elem = lemmatizer.lemmatize(elem)\n",
    "            elem = word_tokenize(elem)\n",
    "            elem = [word.lower() for word in elem if word not in stop_words and word.isalpha()]\n",
    "\n",
    "            self.tok_recipe_name.append(elem)\n",
    "            \n",
    "\n",
    "            \n",
    "    def compute_statistics(self):\n",
    "        \"\"\"\n",
    "        Method to compute statistics for the text data\n",
    "        \"\"\"\n",
    "        self.len_cooking = []\n",
    "        for subl in self.tok_cooking_method:\n",
    "            self.len_cooking.append(len(subl))\n",
    "\n",
    "        self.max_len_cooking = max(self.len_cooking)\n",
    "\n",
    "        self.len_ingredients = []\n",
    "        for subl in self.tok_ingredients:\n",
    "            self.len_ingredients.append(len(subl))\n",
    "\n",
    "        self.max_len_ingredients = max(self.len_ingredients)\n",
    "\n",
    "\n",
    "        self.len_recipe_name = []\n",
    "        for subl in self.tok_recipe_name:\n",
    "            self.len_recipe_name.append(len(subl))\n",
    "\n",
    "        self.max_len_recipe_name = max(self.len_recipe_name)\n",
    "\n",
    "        self.ntags = len(self.tags_unique)\n",
    "\n",
    "        self.tags2id = {tag:i for i, tag in enumerate(self.tags_unique)}\n",
    "\n",
    "\n",
    "        self.total_tags = len(self.tags_flat)\n",
    "        self.tag_weights = dict(Counter(self.tags_flat))\n",
    "\n",
    "        self.tag_weights = {self.tags2id[k]:self.total_tags/v for k,v in self.tag_weights.items()}\n",
    "\n",
    "        \n",
    "    def plot_density(self, len_list):\n",
    "        sns.kdeplot(len_list)\n",
    "  \n",
    "        \n",
    "    def prepare_inputs(self, max_cook_len=200, max_ing_len=100, max_rec_len=10, vector_size=100):\n",
    "        \n",
    "        \"\"\"\n",
    "        Preparing the preprocessed inputs for entry into the classifier\n",
    "        \"\"\"\n",
    "         \n",
    "        if self.vectors_pretrained:\n",
    "            vecs = FastText.load(self.vectors_file)\n",
    "        else:\n",
    "            vecs = model = FastText(ft_data, size=100, window=20, iter=10, workers=3)\n",
    "            vecs.save(\"word_vectors/fasttext.model\")\n",
    "\n",
    "        self.vectors = vecs.wv\n",
    "        del vecs\n",
    "\n",
    "        for i, subl in enumerate(self.tok_cooking_method):\n",
    "            subl = subl[:max_cook_len]\n",
    "            self.tok_cooking_method[i] = subl\n",
    "\n",
    "        for i, subl in enumerate(self.tok_ingredients):\n",
    "            subl = subl[:max_ing_len]\n",
    "            self.tok_ingredients[i] = subl\n",
    "\n",
    "        for i, subl in enumerate(self.tok_recipe_name):\n",
    "            subl = subl[:max_rec_len]\n",
    "            self.tok_recipe_name[i] = subl\n",
    "\n",
    "        self.tok_concat = []\n",
    "\n",
    "        for i, j, k in zip(self.tok_cooking_method, self.tok_ingredients, self.tok_recipe_name):\n",
    "            self.tok_concat.append(i + j + k)\n",
    "\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            self.total_size = len(self.len_cooking)\n",
    "\n",
    "            self.train_indices = list(range(0,int(self.total_size*0.8)-1))\n",
    "            self.train_size = len(self.train_indices)\n",
    "\n",
    "            self.val_indices = list(range(max(self.train_indices), self.total_size))\n",
    "            self.val_size = len(self.val_indices)\n",
    "\n",
    "            self.max_text_len = max_cook_len + max_ing_len + max_rec_len\n",
    "            self.vector_size = vector_size\n",
    "\n",
    "            self.X_train = np.zeros((self.train_size, self.max_text_len, self.vector_size), dtype=K.floatx())\n",
    "\n",
    "            self.X_val = np.zeros((self.val_size, self.max_text_len, self.vector_size), dtype=K.floatx())\n",
    "\n",
    "\n",
    "            for index in tqdm(range(0, self.total_size)):\n",
    "                for t, token in enumerate(self.tok_concat[index]):\n",
    "                    if t >= self.max_text_len:\n",
    "                        break\n",
    "\n",
    "\n",
    "                    if index < self.train_size:\n",
    "                        self.X_train[index, t, :] = self.vectors[token]\n",
    "\n",
    "                    else:\n",
    "                        self.X_val[index-self.train_size, t, :] = self.vectors[token]\n",
    "\n",
    "            self.y = np.zeros((self.total_size, self.ntags), dtype=np.int8)\n",
    "\n",
    "            for i, tag_list in enumerate(self.tags):\n",
    "                for j, tag in enumerate(tag_list):\n",
    "                    if tag in self.tags2id.keys():\n",
    "                        k = self.tags2id[tag]\n",
    "                        self.y[i][k] = 1\n",
    "\n",
    "\n",
    "            self.y_train = self.y[:self.train_size]\n",
    "            self.y_val = self.y[self.train_size-1:]\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.total_size = len(self.len_cooking)\n",
    "\n",
    "            self.max_text_len = max_cook_len + max_ing_len + max_rec_len\n",
    "            self.vector_size = vector_size\n",
    "\n",
    "            self.X_val = np.zeros((self.total_size, self.max_text_len, self.vector_size), dtype=K.floatx())\n",
    "\n",
    "\n",
    "            for index in tqdm(range(0, self.total_size)):\n",
    "                for t, token in enumerate(self.tok_concat[index]):\n",
    "                    if t >= self.max_text_len:\n",
    "                        break\n",
    "\n",
    "                    self.X_val[index, t, :] = self.vectors[token]\n",
    "\n",
    "            self.y = np.zeros((self.total_size, self.ntags), dtype=np.int8)\n",
    "\n",
    "\n",
    "            for i, tag_list in enumerate(self.tags):\n",
    "                for j, tag in enumerate(tag_list):\n",
    "                    if tag in self.tags2id.keys():\n",
    "                        k = self.tags2id[tag]\n",
    "                        self.y[i][k] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "        \n",
    "    def train_model(self, batch_size=32, nb_epochs=100):\n",
    "        \"\"\"\n",
    "        Training the model with set parameters\n",
    "        \"\"\"\n",
    "\n",
    "        input_ = Input(shape=(self.max_text_len,self.vector_size))\n",
    "\n",
    "        model = TimeDistributed(Dense(256, use_bias=False, activation='elu'))(input_)\n",
    "        model = Bidirectional(LSTM(units=50, recurrent_dropout=0.1, return_state = True, return_sequences=True))(model)\n",
    "        model = Bidirectional(LSTM(units=50, recurrent_dropout=0.2))(model)\n",
    "        model = Dense(64, activation=\"elu\")(model)\n",
    "        model = Dropout(0.1)(model)\n",
    "        out = Dense(self.ntags, activation=\"sigmoid\")(model)\n",
    "        self.model = Model(input_, out)\n",
    "        \n",
    "        if self.training == False:\n",
    "            self.model.load_weights(\"model_new_vector.h5\")\n",
    "            return\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                          optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "        self.history = self.model.fit(self.X_train, self.y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      epochs=nb_epochs,\n",
    "                      validation_data=(self.X_val, self.y_val),\n",
    "                      callbacks=[EarlyStopping(min_delta=0.00025, patience=2,\n",
    "                                               monitor='val_loss')],\n",
    "                      class_weight = self.tag_weights,\n",
    "                      verbose=1) \n",
    "        \n",
    "        self.model.save_weights(\"model_new_vector.h5\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, batch_size=32):\n",
    "        \n",
    "        \"\"\"\n",
    "        Generating predictions on the validation dataset (validation = 100% of data if self.training=False)\n",
    "        \"\"\"\n",
    "\n",
    "        self.preds = self.model.predict(self.X_val, batch_size=32)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the DataGetter class into an object. \n",
    "\n",
    "We set training = True and vectors_pretrained = True because we are training a model and using pre-existing word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataGetter(df, training=True, vectors_pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess inputs like so - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63506/63506 [02:10<00:00, 488.18it/s]\n",
      "100%|██████████| 63506/63506 [00:59<00:00, 1074.68it/s]\n",
      "100%|██████████| 63506/63506 [00:18<00:00, 3509.01it/s]\n"
     ]
    }
   ],
   "source": [
    "data.preprocess_inputs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in',\n",
       "  'food',\n",
       "  'processor',\n",
       "  'pulse',\n",
       "  'basil',\n",
       "  'garlic',\n",
       "  'parmesan',\n",
       "  'cheese',\n",
       "  'salt',\n",
       "  'pepper',\n",
       "  'smooth',\n",
       "  'add',\n",
       "  'olive',\n",
       "  'oil',\n",
       "  'pulsing',\n",
       "  'set',\n",
       "  'aside',\n",
       "  'spiralize',\n",
       "  'zucchini',\n",
       "  'cut',\n",
       "  'smaller',\n",
       "  'strands',\n",
       "  'long',\n",
       "  'place',\n",
       "  'work',\n",
       "  'bowl',\n",
       "  'toss',\n",
       "  'pesto',\n",
       "  'tomatoes',\n",
       "  'season',\n",
       "  'salt',\n",
       "  'pepper',\n",
       "  'needed']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tok_cooking_method[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute statistics like so - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "data.compute_statistics()\n",
    "\n",
    "# Number of tags \n",
    "print(data.ntags)\n",
    "\n",
    "\n",
    "# Max length of a cooking_method observation\n",
    "print(data.max_len_cooking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare inputs for entry into the classifier like so - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prepare_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model based on the architecture defined within the class (viewable in data.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 310, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 310, 256)     25600       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 310, 100), ( 122800      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100)          60400       bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           6464        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 776)          50440       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 265,704\n",
      "Trainable params: 265,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 50803 samples, validate on 12704 samples\n",
      "Epoch 1/100\n",
      "50803/50803 [==============================] - 3175s 63ms/step - loss: 74.2972 - acc: 0.9710 - val_loss: 0.0467 - val_acc: 0.9886\n",
      "Epoch 2/100\n",
      "50803/50803 [==============================] - 3139s 62ms/step - loss: 35.3943 - acc: 0.9886 - val_loss: 0.0467 - val_acc: 0.9886\n",
      "Epoch 3/100\n",
      "50803/50803 [==============================] - 3157s 62ms/step - loss: 35.2290 - acc: 0.9886 - val_loss: 0.0470 - val_acc: 0.9886\n"
     ]
    }
   ],
   "source": [
    "data.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating predictions from the validation data (validation data is 100% of the data if training is set to False during instantiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0102352 , 0.01021497, 0.01067653, ..., 0.01037784, 0.00013124,\n",
       "        0.01604219],\n",
       "       [0.00927966, 0.00927555, 0.00982   , ..., 0.00944765, 0.00011345,\n",
       "        0.01470061],\n",
       "       [0.00972946, 0.00967388, 0.01021533, ..., 0.00990782, 0.00012514,\n",
       "        0.01532222],\n",
       "       ...,\n",
       "       [0.0095132 , 0.009451  , 0.00994364, ..., 0.00975599, 0.00011653,\n",
       "        0.01495697],\n",
       "       [0.01049462, 0.0102013 , 0.01072578, ..., 0.01048876, 0.00014113,\n",
       "        0.01587533],\n",
       "       [0.04395235, 0.03795771, 0.04023674, ..., 0.03973234, 0.00224399,\n",
       "        0.04695705]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on random observations from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags the classifier got right for index 4789: 1\n",
      "Number of tags predicted for index 4789: 11\n",
      "Number of actual tags in index 4789: 2\n"
     ]
    }
   ],
   "source": [
    "testindex = 4789\n",
    "\n",
    "test_preds = data.preds.copy()\n",
    "\n",
    "test_preds[test_preds >= 0.1] = 1\n",
    "test_preds[test_preds < 0.1] = 0\n",
    "\n",
    "print(\"Number of tags the classifier got right for index {}: {}\".format(testindex, \n",
    "                                                                        np.logical_and(test_preds[testindex]==1, data.y_val[testindex]==1).sum()))\n",
    "\n",
    "\n",
    "print(\"Number of tags predicted for index {}: {}\".format(testindex, len(np.argwhere(test_preds[testindex]))))\n",
    "\n",
    "print(\"Number of actual tags in index {}: {}\".format(testindex, len(np.argwhere(data.y_val[testindex]))))\n",
    "\n",
    "id2tags = {i: t for t, i in data.tags2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_preds = [pred_val for pred in test_preds for pred_val in pred]\n",
    "\n",
    "flat_y_val = [val for obs in data.y_val for val in obs]\n",
    "\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(flat_y_val, flat_preds, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.24081834914342348\n",
      "Recall: 0.302092213697073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {}\\nRecall: {}\\n\".format(precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted tags: ['Fruit', 'Cheese', 'Dessert', 'Meat', 'Poultry', 'American', 'Main Dish', 'Vegetable', 'Easy', 'Gluten Free', 'Low Sodium']\n",
      "\n",
      "Actual tags: ['Cheese', 'Bread']\n"
     ]
    }
   ],
   "source": [
    "#np.argwhere(test_preds[testindex])\n",
    "\n",
    "pred_tags = [id2tags[i[0]] for i in np.argwhere(test_preds[testindex])]\n",
    "\n",
    "actual_tags = [id2tags[i[0]] for i in np.argwhere(data.y_val[testindex])]\n",
    "\n",
    "print(\"Predicted tags: {}\\n\\nActual tags: {}\".format(pred_tags, actual_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many trials, I could find that the signal is weakly registered in the output probabilities and that there is substantial noise. I haven't had the time to figure out how to solve this issue. To the best of my knowledge, I tried to experiment with several architectures and lowered the threshold for a positive prediction to 0.1 so that more correct items are flagged. \n",
    "\n",
    "Basically, I tried to prioritise recall over precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this already trained model on your own (identical) dataframe, instantiate the DataGetter class and run it like so - \n",
    "\n",
    "We flag training as False and vectors_pretrained as True because we use the pre-existing model and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = DataGetter(your_dataframe, training=False, vectors_pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the steps are the same as for the previous instantiation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nikoml] *",
   "language": "python",
   "name": "conda-env-nikoml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
